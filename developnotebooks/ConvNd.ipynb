{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 14, 14, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvNd(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, bias, padding=0, dilation=1):\n",
    "        \"\"\"\n",
    "        Generalized N-dimensional convolution module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            kernel_size (int or tuple): Size of the convolving kernel.\n",
    "            stride (int or tuple, optional): Stride of the convolution. Default: 1.\n",
    "            padding (int or tuple, optional): Zero-padding added to all sides. Default: 0.\n",
    "            dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n",
    "            bias (bool, optional): If True, adds a learnable bias to the output. Default: True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Ensure kernel_size, stride, padding, and dilation are tuples matching spatial dimensions\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size,)\n",
    "        self.stride = stride if isinstance(stride, tuple) else (stride,) * len(self.kernel_size)\n",
    "        self.padding = padding if isinstance(padding, tuple) else (padding,) * len(self.kernel_size)\n",
    "        self.dilation = dilation if isinstance(dilation, tuple) else (dilation,) * len(self.kernel_size)\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, *self.kernel_size)\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(out_channels))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the generalized N-dimensional convolution.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, *spatial_dims).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, out_channels, *spatial_dims_out).\n",
    "        \"\"\"\n",
    "        batch_size, in_channels, *spatial_dims = x.shape\n",
    "        ndim = len(spatial_dims)\n",
    "\n",
    "        # Adjust padding, stride, and dilation to match the number of dimensions\n",
    "        assert len(self.kernel_size) == ndim, \"Kernel size dimensions must match input dimensions\"\n",
    "\n",
    "        # Pad the input tensor\n",
    "        pad = [(p, p) for p in reversed(self.padding)]  # Reverse to get correct padding order\n",
    "        pad = [item for sublist in pad for item in sublist]  # Flatten padding list\n",
    "        x_padded = torch.nn.functional.pad(x, pad, mode='constant', value=0)\n",
    "\n",
    "        # Compute output spatial dimensions\n",
    "        out_dims = [\n",
    "            (spatial_dims[i] + 2 * self.padding[i] - self.dilation[i] * (self.kernel_size[i] - 1) - 1) // self.stride[i] + 1\n",
    "            for i in range(ndim)\n",
    "        ]\n",
    "\n",
    "        # Extract patches using unfold (this is done sequentially for each spatial dimension)\n",
    "        patches = x_padded.unfold(2, self.kernel_size[0], self.stride[0])\n",
    "        for i in range(1, ndim):\n",
    "            patches = patches.unfold(2 + i, self.kernel_size[i], self.stride[i])\n",
    "\n",
    "        # Reshape patches to prepare for convolution\n",
    "        patches = patches.contiguous().view(batch_size, in_channels, -1, *out_dims)\n",
    "\n",
    "        # Perform convolution via einsum\n",
    "        weight_shape = self.weight.shape\n",
    "        weight = self.weight.view(self.out_channels, self.in_channels, -1)\n",
    "        out = torch.einsum(\"bci...,oci->bo...\", patches, weight)\n",
    "\n",
    "        # Add bias if applicable\n",
    "        if self.bias is not None:\n",
    "            out += self.bias.view(1, -1, *[1] * ndim)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "conv = ConvNd(1, 32, kernel_size=(3, 3, 3, 3), stride=1, bias=False)\n",
    "x = torch.randn(1, 1, 16, 16, 16, 16)  # Example 5D input\n",
    "y = conv(x)\n",
    "print(y.shape)  # Output tensor shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvTransposeNd(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, bias, padding=0, dilation=1):\n",
    "        \"\"\"\n",
    "        Generalized N-dimensional transpose convolution module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            kernel_size (int or tuple): Size of the convolving kernel.\n",
    "            stride (int or tuple, optional): Stride of the convolution. Default: 1.\n",
    "            padding (int or tuple, optional): Zero-padding added to all sides. Default: 0.\n",
    "            dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n",
    "            bias (bool, optional): If True, adds a learnable bias to the output. Default: True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Ensure kernel_size, stride, padding, and dilation are tuples matching spatial dimensions\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size,)\n",
    "        self.stride = stride if isinstance(stride, tuple) else (stride,) * len(self.kernel_size)\n",
    "        self.padding = padding if isinstance(padding, tuple) else (padding,) * len(self.kernel_size)\n",
    "        self.dilation = dilation if isinstance(dilation, tuple) else (dilation,) * len(self.kernel_size)\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(in_channels, out_channels, *self.kernel_size)\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(out_channels))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the generalized N-dimensional transpose convolution.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, *spatial_dims).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, out_channels, *spatial_dims_out).\n",
    "        \"\"\"\n",
    "        batch_size, in_channels, *spatial_dims = x.shape\n",
    "        ndim = len(spatial_dims)\n",
    "\n",
    "        # Compute the output spatial dimensions\n",
    "        out_dims = [\n",
    "            (spatial_dims[i] - 1) * self.stride[i] - 2 * self.padding[i] + self.dilation[i] * (self.kernel_size[i] - 1) + 1\n",
    "            for i in range(ndim)\n",
    "        ]\n",
    "\n",
    "        # Upsample the input tensor by the stride using nearest neighbor interpolation\n",
    "        x_upsampled = x\n",
    "        for i in range(ndim):\n",
    "            x_upsampled = torch.nn.functional.interpolate(\n",
    "                x_upsampled, scale_factor=self.stride[i], mode=\"nearest\"\n",
    "            )\n",
    "\n",
    "        # Apply unfolding to simulate transpose convolution\n",
    "        unfolded = torch.nn.functional.unfold(x_upsampled, self.kernel_size, dilation=self.dilation, padding=self.padding)\n",
    "        unfolded = unfolded.view(batch_size, in_channels, *out_dims)\n",
    "\n",
    "        # Perform the transpose convolution operation by multiplying with the weight\n",
    "        weight = self.weight.view(self.out_channels, self.in_channels, -1)\n",
    "        out = torch.einsum(\"bci...,oci->bo...\", unfolded, weight)\n",
    "\n",
    "        # Add bias if applicable\n",
    "        if self.bias is not None:\n",
    "            out += self.bias.view(1, -1, *[1] * ndim)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Input Error: Only 3D, 4D and 5D input Tensors supported (got 6D) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact (got nearest)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m conv_transpose \u001b[38;5;241m=\u001b[39m ConvTransposeNd(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)  \u001b[38;5;66;03m# Example 6D input tensor\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mconv_transpose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should print a tensor with larger spatial dimensions\u001b[39;00m\n",
      "File \u001b[0;32m/data/storage8/prins/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[42], line 59\u001b[0m, in \u001b[0;36mConvTransposeNd.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m x_upsampled \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ndim):\n\u001b[0;32m---> 59\u001b[0m     x_upsampled \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_upsampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Apply unfolding to simulate transpose convolution\u001b[39;00m\n\u001b[1;32m     64\u001b[0m unfolded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39munfold(x_upsampled, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size, dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding)\n",
      "File \u001b[0;32m/data/storage8/prins/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:3982\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 5D input, but bilinear mode needs 4D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3982\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   3983\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Error: Only 3D, 4D and 5D input Tensors supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3984\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mD) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3985\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), mode)\n\u001b[1;32m   3986\u001b[0m )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Input Error: Only 3D, 4D and 5D input Tensors supported (got 6D) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact (got nearest)"
     ]
    }
   ],
   "source": [
    "conv_transpose = ConvTransposeNd(32, 1, kernel_size=(3, 3, 3, 3), stride=2, bias=False)\n",
    "x = torch.randn(1, 32, 8, 8, 8, 8)  # Example 6D input tensor\n",
    "y = conv_transpose(x)\n",
    "print(y.shape)  # Should print a tensor with larger spatial dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
