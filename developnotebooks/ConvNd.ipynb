{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 14, 14, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvNd(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, bias, padding=0, dilation=1):\n",
    "        \"\"\"\n",
    "        Generalized N-dimensional convolution module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            kernel_size (int or tuple): Size of the convolving kernel.\n",
    "            stride (int or tuple, optional): Stride of the convolution. Default: 1.\n",
    "            padding (int or tuple, optional): Zero-padding added to all sides. Default: 0.\n",
    "            dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n",
    "            bias (bool, optional): If True, adds a learnable bias to the output. Default: True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Ensure kernel_size, stride, padding, and dilation are tuples matching spatial dimensions\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size,)\n",
    "        self.stride = stride if isinstance(stride, tuple) else (stride,) * len(self.kernel_size)\n",
    "        self.padding = padding if isinstance(padding, tuple) else (padding,) * len(self.kernel_size)\n",
    "        self.dilation = dilation if isinstance(dilation, tuple) else (dilation,) * len(self.kernel_size)\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, *self.kernel_size)\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(out_channels))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the generalized N-dimensional convolution.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, *spatial_dims).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, out_channels, *spatial_dims_out).\n",
    "        \"\"\"\n",
    "        batch_size, in_channels, *spatial_dims = x.shape\n",
    "        ndim = len(spatial_dims)\n",
    "\n",
    "        # Adjust padding, stride, and dilation to match the number of dimensions\n",
    "        assert len(self.kernel_size) == ndim, \"Kernel size dimensions must match input dimensions\"\n",
    "\n",
    "        # Pad the input tensor\n",
    "        pad = [(p, p) for p in reversed(self.padding)]  # Reverse to get correct padding order\n",
    "        pad = [item for sublist in pad for item in sublist]  # Flatten padding list\n",
    "        x_padded = torch.nn.functional.pad(x, pad, mode='constant', value=0)\n",
    "\n",
    "        # Compute output spatial dimensions\n",
    "        out_dims = [\n",
    "            (spatial_dims[i] + 2 * self.padding[i] - self.dilation[i] * (self.kernel_size[i] - 1) - 1) // self.stride[i] + 1\n",
    "            for i in range(ndim)\n",
    "        ]\n",
    "\n",
    "        # Extract patches using unfold (this is done sequentially for each spatial dimension)\n",
    "        patches = x_padded.unfold(2, self.kernel_size[0], self.stride[0])\n",
    "        for i in range(1, ndim):\n",
    "            patches = patches.unfold(2 + i, self.kernel_size[i], self.stride[i])\n",
    "\n",
    "        # Reshape patches to prepare for convolution\n",
    "        patches = patches.contiguous().view(batch_size, in_channels, -1, *out_dims)\n",
    "\n",
    "        # Perform convolution via einsum\n",
    "        weight_shape = self.weight.shape\n",
    "        weight = self.weight.view(self.out_channels, self.in_channels, -1)\n",
    "        out = torch.einsum(\"bci...,oci->bo...\", patches, weight)\n",
    "\n",
    "        # Add bias if applicable\n",
    "        if self.bias is not None:\n",
    "            out += self.bias.view(1, -1, *[1] * ndim)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "conv = ConvNd(1, 32, kernel_size=(3, 3, 3, 3), stride=1, bias=False)\n",
    "x = torch.randn(1, 1, 16, 16, 16, 16)  # Example 5D input\n",
    "y = conv(x)\n",
    "print(y.shape)  # Output tensor shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
