{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb9ea18-09ca-42ce-965c-6e8de830cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../ml')\n",
    "from customlayers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85cd9fc7-2046-4329-9030-63bf279e26da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*2*2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d8ebcc-fb28-41af-ac41-b7b1a6de7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {'accelerator': 'gpu',\n",
    "           'devices': [0],\n",
    "           'dtype': torch.float64,\n",
    "           'precision': 64,\n",
    "           'loss_terms': [F.mse_loss],\n",
    "           'loss_coeffs': [1],\n",
    "           'optimizer': torch.optim.Adam, \n",
    "           'learning_rate': 1e-4,\n",
    "           'batch_size': 100,\n",
    "           'max_epochs': 5000,\n",
    "           'early_stopping_patience': 1000000,\n",
    "           'DeepONet': False,\n",
    "           'VarMiON': True,\n",
    "           'NGO': False,\n",
    "           'Q': 12,\n",
    "           'Q_L': 268,\n",
    "           'h': 64, \n",
    "           'Petrov-Galerkin': False,\n",
    "           'bias_NLBranch': False,\n",
    "           'bias_LBranch': False,\n",
    "           'NLB_outputactivation': nn.Tanhshrink(),\n",
    "           'Cholesky': False,\n",
    "           'scale_invariance': False,\n",
    "           'norm_basis': True,\n",
    "           'bound_mus': False,\n",
    "           '1/theta': False,\n",
    "           'symgroupavg': False,\n",
    "           'NOMAD': False}\n",
    "\n",
    "params = {}\n",
    "params['hparams'] = {}\n",
    "params['hparams'] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd67a76-d029-4744-b2a0-af86c4d0173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.hparams = params['hparams']\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.num_channels = 30\n",
    "        # Adjusted convolutional layers\n",
    "        self.layers.append(ReshapeLayer(output_shape=(1,self.hparams['h'],self.hparams['h'])))\n",
    "        self.layers.append(nn.Conv2d(in_channels=1, out_channels=self.num_channels, kernel_size=2, stride=2, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Conv2d(in_channels=self.num_channels, out_channels=2*self.num_channels, kernel_size=2, stride=2, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Conv2d(in_channels=2*self.num_channels, out_channels=1*self.num_channels, kernel_size=2, stride=2, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Conv2d(in_channels=1*self.num_channels, out_channels=1, kernel_size=2, stride=2, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(ReshapeLayer(output_shape=(int(1*(self.hparams['h']/16)**2),)))\n",
    "        self.layers.append(nn.Linear(in_features=int(1*(self.hparams['h']/16)**2),out_features=int(1*(self.hparams['h']/16)**2)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(ReshapeLayer(output_shape=(1,int(self.hparams['h']/16),int(self.hparams['h']/16))))\n",
    "        self.layers.append(nn.ConvTranspose2d(1, self.num_channels, kernel_size=2, stride=2, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.ConvTranspose2d(self.num_channels, 2*self.num_channels, kernel_size=2, stride=2, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.ConvTranspose2d(2*self.num_channels, 1*self.num_channels, kernel_size=2, stride=2, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.ConvTranspose2d(1*self.num_channels, 1, kernel_size=2, stride=2, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(ReshapeLayer(output_shape=(self.hparams['h'],self.hparams['h'])))\n",
    "        if self.hparams['NLB_outputactivation'] is not None:\n",
    "            self.layers.append(self.hparams['NLB_outputactivation'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        x1 = self.layers[0](x0)\n",
    "        x2 = self.layers[1](x1)\n",
    "        x3 = self.layers[2](x2)\n",
    "        x4 = self.layers[3](x3)\n",
    "        x5 = self.layers[4](x4)\n",
    "        x6 = self.layers[5](x5)\n",
    "        x7 = self.layers[6](x6)\n",
    "        x8 = self.layers[7](x7)\n",
    "        x9 = self.layers[8](x8)\n",
    "        x10 = self.layers[9](x9)\n",
    "        x11 = self.layers[10](x10) + x9\n",
    "        x12 = self.layers[11](x11)\n",
    "        x13 = self.layers[12](x12)\n",
    "        x14 = self.layers[13](x13) + x7\n",
    "        x15 = self.layers[14](x14)\n",
    "        x16 = self.layers[15](x15) + x5\n",
    "        x17 = self.layers[16](x16)\n",
    "        x18 = self.layers[17](x17) + x3\n",
    "        x19 = self.layers[18](x18) + x1\n",
    "        x20 = self.layers[19](x19)\n",
    "        x21 = self.layers[20](x20)\n",
    "        y = x21\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb5d1c39-00bc-4a45-8079-79c42a94be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(hparams['batch_size'],hparams['h'],hparams['h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86fbff36-df28-46c6-8a23-1a2fad01eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b1636b-a46e-47ab-92b0-9703ecf3ecab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29552"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77cd833b-b433-4920-88b3-1000bf927a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2804e-02,  3.8973e-03,  1.3876e-01,  ...,  3.9982e-02,\n",
       "           3.7415e-01,  1.9780e-01],\n",
       "         [ 4.0778e-02,  8.2344e-03,  9.5532e-03,  ...,  2.1135e-01,\n",
       "           1.0635e-01,  4.5960e-04],\n",
       "         [ 3.9281e-02,  8.3237e-01,  8.4772e-02,  ...,  1.6979e-01,\n",
       "           1.7192e-04,  3.6278e-01],\n",
       "         ...,\n",
       "         [ 9.1115e-02,  5.9623e-06,  9.8518e-03,  ...,  2.0320e-01,\n",
       "           1.0824e-01,  1.1338e-01],\n",
       "         [ 9.1620e-03,  3.3493e-01,  1.3121e-01,  ...,  4.9063e-01,\n",
       "           5.4177e-05,  2.9967e-02],\n",
       "         [ 5.2140e-02,  1.3117e-02,  9.5292e-02,  ..., -4.8058e-04,\n",
       "           7.7246e-03,  5.8889e-03]],\n",
       "\n",
       "        [[ 2.3735e-03,  3.4624e-01,  3.9858e-03,  ...,  4.5389e-01,\n",
       "           2.4706e-02,  3.7337e-02],\n",
       "         [ 4.7612e-04, -2.8990e-05,  6.5114e-02,  ...,  1.2342e-01,\n",
       "           5.3806e-03, -2.8033e-07],\n",
       "         [ 3.0117e-01,  6.3709e-01,  2.8061e-01,  ...,  3.2392e-01,\n",
       "           1.7693e-01,  3.9921e-01],\n",
       "         ...,\n",
       "         [ 1.2515e-03,  5.2462e-02,  1.5724e-01,  ...,  4.4234e-02,\n",
       "           1.6314e-02,  2.7342e-01],\n",
       "         [ 1.3295e-01,  1.3900e-01,  1.4808e-07,  ...,  4.5867e-03,\n",
       "           2.7158e-01,  3.1496e-01],\n",
       "         [ 1.0334e-02,  9.5585e-02,  1.2215e-02,  ...,  1.2977e-01,\n",
       "           2.8762e-02,  9.5707e-02]],\n",
       "\n",
       "        [[ 1.2176e-02,  3.5640e-01,  1.0426e-01,  ...,  6.0282e-01,\n",
       "           1.0328e-01,  9.4652e-02],\n",
       "         [ 2.5312e-02,  6.1797e-03,  1.9515e-01,  ...,  1.1114e-02,\n",
       "           3.0233e-03, -1.9759e-05],\n",
       "         [ 1.7238e-01,  1.1203e-03,  6.1440e-04,  ...,  2.6671e-01,\n",
       "           3.3953e-01,  3.5205e-01],\n",
       "         ...,\n",
       "         [ 1.6804e-01,  7.4169e-02,  8.3524e-02,  ...,  3.9165e-02,\n",
       "           1.7839e-01,  9.8919e-02],\n",
       "         [ 6.7547e-03,  1.2153e-03,  4.1749e-01,  ...,  3.3048e-02,\n",
       "           1.3708e-01,  7.0949e-01],\n",
       "         [ 5.6025e-03,  1.4042e-01,  2.7459e-01,  ...,  2.1591e-02,\n",
       "           2.2046e-01,  3.6979e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 7.8893e-02,  7.7567e-04,  5.6121e-06,  ...,  1.2293e-01,\n",
       "           1.6090e-02,  1.4265e-01],\n",
       "         [ 1.5708e-02,  3.0254e-01,  1.4098e-04,  ...,  1.9827e-02,\n",
       "           2.5473e-01,  1.7353e-03],\n",
       "         [ 1.0023e-01,  1.4560e-01,  1.3262e-06,  ...,  6.6339e-01,\n",
       "           1.3203e-01,  2.8359e-01],\n",
       "         ...,\n",
       "         [ 1.9949e-02, -4.7480e-03,  1.3726e-02,  ...,  1.2951e-04,\n",
       "           1.5470e-03, -4.0206e-04],\n",
       "         [ 6.9849e-03,  3.1596e-03,  3.7490e-03,  ...,  3.1286e-04,\n",
       "           6.2487e-02,  5.0230e-02],\n",
       "         [ 5.9650e-02,  6.3729e-03,  4.8862e-03,  ...,  2.9733e-04,\n",
       "           1.2966e-04,  4.1674e-04]],\n",
       "\n",
       "        [[ 1.7299e-01,  1.5614e-01,  2.6908e-01,  ...,  1.2367e-02,\n",
       "           6.6767e-02,  1.5472e-02],\n",
       "         [ 1.7881e-07,  7.4339e-04,  5.3722e-03,  ...,  2.0740e-01,\n",
       "           1.6054e-01,  2.7089e-03],\n",
       "         [ 2.8856e-01,  4.8334e-01,  1.8631e-04,  ...,  6.8894e-03,\n",
       "           8.2610e-02,  5.5821e-01],\n",
       "         ...,\n",
       "         [ 3.7956e-04,  3.4450e-03,  1.7754e-02,  ...,  2.7710e-02,\n",
       "           4.4408e-02,  1.5623e-02],\n",
       "         [ 7.4517e-02,  5.9362e-01,  3.2168e-06,  ...,  5.2774e-01,\n",
       "           4.9882e-06,  2.8924e-01],\n",
       "         [ 1.8318e-01,  5.9861e-02,  1.5319e-01,  ...,  1.9643e-05,\n",
       "           7.8636e-02,  2.6494e-02]],\n",
       "\n",
       "        [[ 2.2795e-01,  4.5978e-02,  4.0424e-04,  ...,  4.6961e-01,\n",
       "           5.4095e-05,  4.4274e-01],\n",
       "         [ 2.9209e-01,  1.5226e-01,  1.0885e-03,  ...,  1.2851e-01,\n",
       "           8.1582e-02, -6.9875e-05],\n",
       "         [ 5.0747e-03,  8.3124e-03,  8.5466e-05,  ...,  4.9711e-02,\n",
       "           4.0657e-01,  5.9320e-01],\n",
       "         ...,\n",
       "         [ 1.2191e-02,  8.4843e-02,  1.4240e-02,  ...,  2.1227e-01,\n",
       "           4.0088e-02,  1.4293e-01],\n",
       "         [ 5.3508e-03,  4.0690e-01,  1.4215e-01,  ...,  4.3531e-01,\n",
       "           1.6619e-01,  3.3936e-01],\n",
       "         [ 1.1389e-02,  1.2439e-02,  1.3255e-03,  ...,  4.1436e-03,\n",
       "           7.6312e-02,  3.6864e-02]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc09f8d-4e12-4638-b873-926bf97c4df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*4*3*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "353d3e8a-0b83-4acc-a168-c893bfb9b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.hparams = params['hparams']\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.num_channels = 30\n",
    "        self.kernel_sizes = [4,4,3,3]\n",
    "        # Adjusted convolutional layers\n",
    "        self.layers.append(ReshapeLayer(output_shape=(1,self.hparams['h'],self.hparams['h'])))\n",
    "        self.layers.append(nn.Conv2d(in_channels=1, out_channels=self.num_channels, kernel_size=4, stride=4, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.BatchNorm2d(num_features=self.num_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        self.layers.append(nn.Conv2d(in_channels=self.num_channels, out_channels=2*self.num_channels, kernel_size=4, stride=4, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.BatchNorm2d(num_features=2*self.num_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        self.layers.append(nn.Conv2d(in_channels=2*self.num_channels, out_channels=self.num_channels, kernel_size=3, stride=3, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Conv2d(in_channels=self.num_channels, out_channels=1, kernel_size=3, stride=3, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(ReshapeLayer(output_shape=(int(1*(self.hparams['h']/144)**2),)))\n",
    "        self.layers.append(nn.Linear(in_features=int(1*(self.hparams['h']/144)**2),out_features=int(1*(self.hparams['h']/144)**2)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(ReshapeLayer(output_shape=(1,int(self.hparams['h']/144),int(self.hparams['h']/144))))\n",
    "        self.layers.append(nn.ConvTranspose2d(1, self.num_channels, kernel_size=3, stride=3, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.BatchNorm2d(num_features=self.num_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        self.layers.append(nn.ConvTranspose2d(self.num_channels, 2*self.num_channels, kernel_size=3, stride=3, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.BatchNorm2d(num_features=2*self.num_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        self.layers.append(nn.ConvTranspose2d(2*self.num_channels, self.num_channels, kernel_size=4, stride=4, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.ConvTranspose2d(self.num_channels, 1, kernel_size=4, stride=4, bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(ReshapeLayer(output_shape=(self.hparams['h'],self.hparams['h'])))\n",
    "        if self.hparams['NLB_outputactivation'] is not None:\n",
    "            self.layers.append(self.hparams['NLB_outputactivation'])\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        x1 = self.layers[0](x0)\n",
    "        x2 = self.layers[1](x1)\n",
    "        x3 = self.layers[2](x2)\n",
    "        x4 = self.layers[3](x3)\n",
    "        x5 = self.layers[4](x4)\n",
    "        x6 = self.layers[5](x5)\n",
    "        x7 = self.layers[6](x6)\n",
    "        x8 = self.layers[7](x7)\n",
    "        x9 = self.layers[8](x8)\n",
    "        x10 = self.layers[9](x9) \n",
    "        x11 = self.layers[10](x10)\n",
    "        x12 = self.layers[11](x11)\n",
    "        x13 = self.layers[12](x12) + x11\n",
    "        x14 = self.layers[13](x13)\n",
    "        x15 = self.layers[14](x14)\n",
    "        x16 = self.layers[15](x15) + x9\n",
    "        x17 = self.layers[16](x16)\n",
    "        x18 = self.layers[17](x17)\n",
    "        x19 = self.layers[18](x18) + x6\n",
    "        x20 = self.layers[19](x19)\n",
    "        x21 = self.layers[20](x20) \n",
    "        x22 = self.layers[21](x21) + x3\n",
    "        x23 = self.layers[22](x22) + x1\n",
    "        x24 = self.layers[23](x23)\n",
    "        x25 = self.layers[24](x24)\n",
    "        y = x25\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcbad8c5-9753-49c2-b5ef-d0dbb323814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(hparams['batch_size'],hparams['h'],hparams['h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259bf254-5cfc-4f4a-8598-ab8b684c26d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prins/st8/anaconda3/lib/python3.10/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "model = UNet(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde95fa4-40fb-498d-a0c6-ca7d91aed522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91860"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbd88944-ce2d-40e9-8339-f30e2a664398",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (1 x 1). Kernel size: (3 x 3). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 48\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m x8 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m7\u001b[39m](x7)\n\u001b[1;32m     47\u001b[0m x9 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m8\u001b[39m](x8)\n\u001b[0;32m---> 48\u001b[0m x10 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx9\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     49\u001b[0m x11 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m10\u001b[39m](x10)\n\u001b[1;32m     50\u001b[0m x12 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m11\u001b[39m](x11)\n",
      "File \u001b[0;32m~/st8/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/st8/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/st8/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (1 x 1). Kernel size: (3 x 3). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a887e600-1181-4e9a-8fa3-267325ab4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.hparams = params['hparams']\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.num_channels = 24\n",
    "        self.kernel_sizes = [4,4,2,2]\n",
    "        # Adjusted convolutional layers\n",
    "        self.layers.append(ReshapeLayer(output_shape=(1,self.hparams['h'],self.hparams['h'])))\n",
    "        self.layers.append(nn.Conv2d(in_channels=1, out_channels=self.num_channels, kernel_size=self.kernel_sizes[0], stride=self.kernel_sizes[0], bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.BatchNorm2d(num_features=self.num_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        self.layers.append(nn.Conv2d(in_channels=self.num_channels, out_channels=self.num_channels, kernel_size=self.kernel_sizes[1], stride=self.kernel_sizes[1], bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.BatchNorm2d(num_features=self.num_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        self.layers.append(nn.Conv2d(in_channels=self.num_channels, out_channels=self.num_channels, kernel_size=self.kernel_sizes[2], stride=self.kernel_sizes[2], bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Conv2d(in_channels=self.num_channels, out_channels=self.num_channels, kernel_size=self.kernel_sizes[3], stride=self.kernel_sizes[3], bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(ReshapeLayer(output_shape=(int(self.num_channels*(self.hparams['h']/144)**2),)))\n",
    "        self.layers.append(nn.Linear(in_features=int(self.num_channels*(self.hparams['h']/144)**2),out_features=int(self.num_channels*(self.hparams['h']/144)**2)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(ReshapeLayer(output_shape=(self.num_channels,int(self.hparams['h']/144),int(self.hparams['h']/144))))\n",
    "        self.layers.append(nn.ConvTranspose2d(self.num_channels, self.num_channels, kernel_size=self.kernel_sizes[3], stride=self.kernel_sizes[3], bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.BatchNorm2d(num_features=self.num_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        self.layers.append(nn.ConvTranspose2d(self.num_channels, self.num_channels, kernel_size=self.kernel_sizes[2], stride=self.kernel_sizes[2], bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.BatchNorm2d(num_features=self.num_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        self.layers.append(nn.ConvTranspose2d(self.num_channels, self.num_channels, kernel_size=self.kernel_sizes[1], stride=self.kernel_sizes[1], bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.ConvTranspose2d(self.num_channels, 1, kernel_size=self.kernel_sizes[0], stride=self.kernel_sizes[0], bias=self.hparams.get('bias_NLBranch', True)))\n",
    "        self.layers.append(ReshapeLayer(output_shape=(self.hparams['h'],self.hparams['h'])))\n",
    "        if self.hparams['NLB_outputactivation'] is not None:\n",
    "            self.layers.append(self.hparams['NLB_outputactivation'])\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        x1 = self.layers[0](x0)\n",
    "        x2 = self.layers[1](x1)\n",
    "        x3 = self.layers[2](x2)\n",
    "        x4 = self.layers[3](x3)\n",
    "        x5 = self.layers[4](x4)\n",
    "        x6 = self.layers[5](x5)\n",
    "        x7 = self.layers[6](x6)\n",
    "        x8 = self.layers[7](x7)\n",
    "        x9 = self.layers[8](x8)\n",
    "        x10 = self.layers[9](x9) \n",
    "        x11 = self.layers[10](x10)\n",
    "        x12 = self.layers[11](x11)\n",
    "        x13 = self.layers[12](x12) + x11\n",
    "        x14 = self.layers[13](x13)\n",
    "        x15 = self.layers[14](x14)\n",
    "        x16 = self.layers[15](x15) + x9\n",
    "        x17 = self.layers[16](x16)\n",
    "        x18 = self.layers[17](x17)\n",
    "        x19 = self.layers[18](x18) + x6\n",
    "        x20 = self.layers[19](x19)\n",
    "        x21 = self.layers[20](x20) \n",
    "        x22 = self.layers[21](x21) + x3\n",
    "        x23 = self.layers[22](x22) + x1\n",
    "        x24 = self.layers[23](x23)\n",
    "        x25 = self.layers[24](x24)\n",
    "        y = x25\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87a8b65f-8d4b-44cf-9deb-231a9418841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(hparams['batch_size'],hparams['h'],hparams['h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99fd1da4-4a6b-45fd-8772-39d5e28ed445",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e03fc6aa-8bd4-43e5-adb7-8bc7bf288c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28628"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8093af05-0eb2-450b-973a-6cb6b84736ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (1 x 1). Kernel size: (3 x 3). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 48\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m x8 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m7\u001b[39m](x7)\n\u001b[1;32m     47\u001b[0m x9 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m8\u001b[39m](x8)\n\u001b[0;32m---> 48\u001b[0m x10 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx9\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     49\u001b[0m x11 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m10\u001b[39m](x10)\n\u001b[1;32m     50\u001b[0m x12 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m11\u001b[39m](x11)\n",
      "File \u001b[0;32m~/st8/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/st8/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/st8/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (1 x 1). Kernel size: (3 x 3). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77346e18-c827-4c73-b3ee-abb99d575b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
